{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook is for Execution of PromptU \"the Prompt Tuner\" without using Streamlit\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils.docs2data import read_docs_to_dataframe\n",
    "import json\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "import pandas as pd\n",
    "\n",
    "from prompt_optimization.signatures import init_signatures\n",
    "from prompt_optimization.scan_report import ScanReport\n",
    "from prompt_optimization.main import optimize\n",
    "from prompt_optimization.output_postprocessing import aggregate_per_category, flatten_dict, dict_to_df\n",
    "# from prompt_optimization.signatures import SummaryReasoning\n",
    "from utils.validation import substring_metric, lm_metric, substring_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set configs for prompt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_documents = \"business report\" #singular\n",
    "categories = ['FX_HEDGING', 'COMMODITIES_HEDGING', 'INTEREST_RATE_HEDGING', 'CREDIT', 'INSURANCE', 'FACTORING', 'PENSIONS', 'ESG', 'CASH_MANAGEMENT', 'DEPOSITS', 'ASSET_MANAGEMENT', 'OTHER']\n",
    "class_of_categories = \"banking products\" #plural\n",
    "objective = \"extracting company specific information that indicate sales opportunities for products relating to capital market or asset management\" # present progressive\n",
    "metric = 'simple' #'llm'\n",
    "optimizer = 'combined' # 'bootstrap-few-shot'\n",
    "number_of_items_in_output = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call PomptU model creation and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path, model_number = optimize(type_of_documents, categories, class_of_categories, objective, metric, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(os.path.dirname('/home/cdsw/CB2MUHX/project-weeks-promptu/models'))\n",
    "sys.path.append(os.path.dirname('/home/cdsw/CB2LOI5/project-weeks-promptu/models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize LLM, when optimizer is not used\n",
    "from module.azure_openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables from .ini file\n",
    "from configparser import ConfigParser\n",
    "config_object = ConfigParser()\n",
    "config_object.read(\"config.ini\")\n",
    "tud_api_key = config_object[\"TUD_API_KEY\"]['tud_api_key']\n",
    "dev_api_key = config_object[\"DEV_API_KEY\"]['dev_api_key']\n",
    "\n",
    "lm_gpt = AzureOpenAI(\n",
    "    tud_dev = \"TUD\",\n",
    "    api_version = '2024-06-01', #'2024-06-01',#'2023-07-01-preview',\n",
    "    model_name = \"gpt-4o\", \n",
    "    api_key = tud_api_key,\n",
    "    model_type = \"chat\"\n",
    ")\n",
    "\n",
    "dspy.settings.configure(lm=lm_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df from txt reports\n",
    "folder_path = Path('data/reports/train_annotated/')\n",
    "df = read_docs_to_dataframe(folder_path)\n",
    "    # df = df[df['quote'].apply(len) > 0]\n",
    "    # Remove quotation marks\n",
    "df['context'] = df['context'].str.replace(r'[\"]', '', regex=True)\n",
    "\n",
    "\n",
    "df.to_excel('train_data.xlsx', index=False)\n",
    "\n",
    "training_examples = json.loads(df[[\"context\",\"answer\"]].to_json(orient=\"records\"))\n",
    "train_dspy = [dspy.Example(x).with_inputs('context') for x in training_examples]\n",
    "\n",
    "hint, ChunkerSignature, PredictRelevance, PredictCategory, Translator = init_signatures(type_of_documents, number_of_items_in_output, objective, class_of_categories, categories)\n",
    "loaded_program = ScanReport(hint, ChunkerSignature, PredictRelevance, PredictCategory, Translator)\n",
    "\n",
    "# loaded_program = ScanReport()\n",
    "loaded_program.load(path='models/0124534897.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display answer and make evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullana = loaded_program(train_dspy[8].context)\n",
    "fullana.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=train_dspy[0:13], num_threads=1, display_progress=True, display_table=4)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(loaded_program, metric=substring_metric, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = aggregate_per_category(fullana.answer, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archived Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SummaryReasoning(dspy.Signature):\n",
    "#     __doc__ = f\"\"\"Given a resoning text, summarize and make a meningful conclusion. Do not include the context in the output. Remove introductions and comments.\"\"\"\n",
    "#     context = dspy.InputField()\n",
    "#     output = dspy.OutputField(desc=\"German\")\n",
    "    \n",
    "# # function to aggregate results per category\n",
    "# def aggregate_per_category(result_list: list, categories) -> dict:\n",
    "#     \"\"\"This function aggregates the output results per category. For the relevance score the maximum is taken.\n",
    "#     input:\n",
    "#     df : pd.DataFrame\n",
    "#         Dictionary in json format\n",
    "#     categories\n",
    "#         a list of the categories specified by the user\n",
    "#     output:\n",
    "#     df_agg:\n",
    "#         pd.DataFrame with the aggregated results per category\n",
    "#     \"\"\"\n",
    "\n",
    "#     result_dict = flatten_dict(result_list) # flatten\n",
    "#     df = dict_to_df(result_dict) # convert to df\n",
    "#     categories = df.categories.unique\n",
    "#     print(categories)\n",
    "\n",
    "#     # initialize df_agg\n",
    "#     df_agg=pd.DataFrame(columns = df.columns)\n",
    "\n",
    "#     # Loop through categpries\n",
    "#     for item in categories:\n",
    "#         print(item)\n",
    "#         matched = False\n",
    "#         quote = \"\"\n",
    "#         relevance_score = 0\n",
    "#         reasoning_categories = \"\"\n",
    "#         reasoning_relevance = \"\"\n",
    "#         n=0\n",
    "#         for i in range(len(df)):\n",
    "#             print(df.loc[i].categories)\n",
    "#             # add results if there are some matching\n",
    "#             if item in df.loc[i].categories:\n",
    "#                 print(item)\n",
    "#                 n += 1\n",
    "#                 matched = True\n",
    "#                 relevance_score = max(relevance_score, int(df.loc[i].relevance_score))\n",
    "#                 quote = f\"{quote}# Quote {n}: {str(df.loc[i].quote)}\"\n",
    "#                 reasoning_categories = df.loc[i].reasoning_categories\n",
    "#                 reasoning_relevance = df.loc[i].reasoning_relevance\n",
    "\n",
    "#         if matched:\n",
    "#             new_row = {'quote': quote, 'relevance_score': relevance_score, 'categories': item, 'reasoning_categories': reasoning_categories, 'reasoning_relevance': reasoning_relevance}\n",
    "#             print('New row:', new_row)\n",
    "#             df_agg = pd.concat([df_agg, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "#     print('DF AGG', df_agg)\n",
    "\n",
    "#     # sort values descending by relevance_score\n",
    "#     df_agg = df_agg.sort_values(by=['relevance_score'], ascending=False)\n",
    "\n",
    "#     print('DF AGG', df_agg)\n",
    "\n",
    "#     summary_reasoning_categories = []\n",
    "#     summary_reasoning_relevance = []\n",
    "\n",
    "#     summarize_reasoning = dspy.Predict(SummaryReasoning)\n",
    "#     for i in range(len(df_agg)):\n",
    "#         summary_reasoning_categories.append(extract_output(summarize_reasoning(context = df_agg.loc[i]['reasoning_categories']).output))\n",
    "#         summary_reasoning_relevance.append(extract_output(summarize_reasoning(context = df_agg.loc[i]['reasoning_relevance']).output))\n",
    "    \n",
    "#     df_agg['reasoning_categories'] = summary_reasoning_categories\n",
    "#     df_agg['reasoning_relevance'] = summary_reasoning_relevance\n",
    "\n",
    "#     print('DF AGG', df_agg)\n",
    "\n",
    "#     print('df t', df_agg.transpose())\n",
    "    \n",
    "#     dict_agg = df_agg.transpose().to_dict() # convert to dict\n",
    "#     print(dict_agg)\n",
    "    \n",
    "#     return dict_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_categories_res = {}\n",
    "\n",
    "# summarize_reasoning = dspy.Preduct(SummaryReasoning)\n",
    "# for i in len(result_dict):\n",
    "#         summary_categories_res[i] = summarize_reasoning(result_dict[i].reasoning_categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
