{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original Prompt from Swante:\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "# You are a bank employee, relationship manager for corporate clients. You will be given\n",
    "#     an annual report in the user prompt.\n",
    "#     Instructions:\n",
    "#     - Please extract sales leads (i.e. ideas for bank products that might be interesting for the\n",
    "#     customer) from the text, if any exist.\n",
    "#     - Only provide bank product recommentations as sales leads.\n",
    "#     - Focus on bank products related to capital markets / asset management such as\n",
    "#       commodities, currencies, interest rates, pensions and cash investments.\n",
    "#     - Only provide the sales leads along with the reasons for that in your answer.\n",
    "#     - Provide the sentences in the annual report the sales leads are deduced from.\n",
    "#     - If no sales lead can be deduced from the annual report, answer accordingly.\n",
    "#     - Only provide the sales leads with the highest probability, at maximum five.\n",
    "#     - Only provide sales leads that are specific for the company.\n",
    "#     - Answer in German.\n",
    "#     - Provide the answer in bullet points in the following schema:\n",
    "#     <Produkt>\n",
    "#         - Grund: ...\n",
    "#         - Zitat: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from module.azure_openai import AzureOpenAI\n",
    "import httpx\n",
    "from datetime import datetime\n",
    "import getpass\n",
    "import dspy\n",
    "from dspy import dsp\n",
    "from dspy import evaluate\n",
    "from dspy.primitives import Example\n",
    "from teleprompt.teleprompt import Teleprompter\n",
    "from teleprompt.vanilla import LabeledFewShot\n",
    "from utils.docs2data import read_docs_to_dataframe\n",
    "from utils.validation import text_preprocessing, quote_match, categories_match, AssessReasoning, relevance_reasoning_lm, AssessQuote, quote_match_lm, AssessCategories, categories_match_lm\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# Load environment variables from .ini file\n",
    "from configparser import ConfigParser\n",
    "config_object = ConfigParser()\n",
    "config_object.read(\"config.ini\")\n",
    "tud_api_key = config_object[\"TUD_API_KEY\"]['tud_api_key']\n",
    "dev_api_key = config_object[\"DEV_API_KEY\"]['dev_api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and create training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df from txt reports\n",
    "folder_path = Path('data/reports/train_annotated/')\n",
    "df = read_docs_to_dataframe(folder_path)\n",
    "# df = df[df['quote'].apply(len) > 0]\n",
    "# Remove quotation marks\n",
    "df['context'] = df['context'].str.replace(r'[\"]', '', regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[8].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df from txt reports\n",
    "folder_path = Path('data/reports/validation_annotated/')\n",
    "df_val = read_docs_to_dataframe(folder_path)\n",
    "# df_val = df_val[df_val['quote'].apply(len) > 0]\n",
    "# Remove quotation marks\n",
    "df_val['context'] = df_val['context'].str.replace(r'[\"]', '', regex=True)\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val.iloc[4].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dspy training dataset\n",
    "training_examples = json.loads(df[[\"context\",\"answer\"]].to_json(orient=\"records\"))\n",
    "train_dspy = [dspy.Example(x).with_inputs('context') for x in training_examples]\n",
    "print(f\"For this dataset, training examples have input keys {train_dspy[0].inputs().keys()} and label keys {train_dspy[0].labels().keys()}\")\n",
    "\n",
    "# create dspy validation dataset\n",
    "val_examples = json.loads(df[[\"context\",\"answer\"]].to_json(orient=\"records\"))\n",
    "val_dspy = [dspy.Example(x).with_inputs('context') for x in val_examples]\n",
    "print(f\"For this dataset, validation examples have input keys {val_dspy[0].inputs().keys()} and label keys {val_dspy[0].labels().keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_gpt = AzureOpenAI(\n",
    "    tud_dev = \"TUD\",\n",
    "    api_version = '2024-06-01', #'2024-06-01',#'2023-07-01-preview',\n",
    "    model_name = \"gpt-4o\", \n",
    "    api_key = tud_api_key,\n",
    "    model_type = \"chat\"\n",
    ")\n",
    "\n",
    "dspy.settings.configure(lm=lm_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_documents = \"business report\" #singular\n",
    "categories = ['FX_HEDGING', 'COMMODITIES_HEDGING', 'INTEREST_RATE_HEDGING', 'CREDIT', 'INSURANCE', 'FACTORING', 'PENSIONS', 'ESG', 'CASH_MANAGEMENT', 'DEPOSITS', 'ASSET_MANAGEMENT', 'OTHER']\n",
    "class_of_categories = \"banking products\" #plural\n",
    "#relevance_classes = ['highly relevant', 'relevant', 'less relevant']\n",
    "objective = \"extracting company specific information that indicate sales opportunities for products relating to capital market or asset management\" # present progressive\n",
    "number_of_items_in_output = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkerSignature(dspy.Signature):\n",
    "    __doc__ = f\"\"\"Given a {type_of_documents}, determine {number_of_items_in_output} most relevant snippets (2-3 sentences) to {objective}. Do not include the context in the output.\"\"\"\n",
    "    context = dspy.InputField()\n",
    "    output = dspy.OutputField(desc=\"comma-separated quotes\")\n",
    "\n",
    "class PredictRelevance(dspy.Signature):\n",
    "    __doc__ = f\"\"\"Given a snippet from a {type_of_documents}, determine a score between 0 and 100 of how relevant the snippet is to {objective}. A score of 100 denotes high relevance, and a score 0 denotes irrelevance.\"\"\"\n",
    "    context = dspy.InputField()\n",
    "    output = dspy.OutputField(desc=\"number between 0 and 100\")\n",
    "\n",
    "class PredictCategory(dspy.Signature):\n",
    "    __doc__ = f\"\"\"Given a snippet from a {type_of_documents}, identify which of the {class_of_categories} ({', '.join(categories)}) the snippet is relevant to. If snippet is not relevantv for any {class_of_categories}, say 'other'.\"\"\"\n",
    "    context = dspy.InputField()\n",
    "    output = dspy.OutputField(desc=\"comma-separated {class_of_categories}\", format=lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "class Translator(dspy.Signature):\n",
    "    __doc__ = f\"\"\"Do not include the context and introduction in the output. Translate to German.\"\"\"\n",
    "    context = dspy.InputField()\n",
    "    output = dspy.OutputField(desc=\"German\")\n",
    "\n",
    "def valid_categories(predicted_categories, categories):\n",
    "    \"\"\"check if predcited {class_of_categories} is a valid {class_of_categories}\"\"\"\n",
    "    return all(str(item) in categories for item in predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chunkers import extract_output, extract_reasoning\n",
    "#from utils.chunkers import read_german_abbreviations, chunk_german_multi_sentences\n",
    "#ger_abbr = read_german_abbreviations('utils/german_abbreviations.txt')\n",
    "# Grounding with prior\n",
    "HINT = \"Valid {class_of_categories} are:\" \n",
    "hint = f\"{HINT} {', '.join(categories)}.\" if categories else None\n",
    "\n",
    "class ScanReport(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # devides report into chunks\n",
    "        #self.chunk = chunk_german_multi_sentences(abbreviations=abbreviations, sentences_per_chunk=sentences_per_chunk, overlap=overlap)\n",
    "        # preselect relevant snippets\n",
    "        self.preselection = dspy.Predict(ChunkerSignature)\n",
    "        # given an annual report snippet, rate relevance to information extraction\n",
    "        self.relevance = dspy.ChainOfThought(PredictRelevance)\n",
    "        # given a snippet, predict a list of relevant categories using a CoT\n",
    "        self.predict = dspy.ChainOfThoughtWithHint(PredictCategory)\n",
    "        # reduce the number of extracted infos\n",
    "        self.translator = dspy.Predict(Translator)\n",
    "\n",
    "    def forward(self, context):\n",
    "        hint = f\"{HINT} {', '.join(categories)}.\"\n",
    "        answers = []\n",
    "        reasoning = []\n",
    "        \n",
    "        preselection = self.preselection(context=context)\n",
    "        # print([item.replace('\"', '') for item in extract_output(preselection.output).split('\", \"')])\n",
    "\n",
    "        # for each chunk in the preselection\n",
    "        for snippet in [item.replace('\"', '') for item in extract_output(preselection.output).split('\", \"')]:\n",
    "            # use the LM to predict relevant products\n",
    "            chunk_categories = self.predict(context=[snippet], hint=hint)\n",
    "\n",
    "            chunk_relevance = self.relevance(context=[snippet])\n",
    "            entry = {\n",
    "                \"quote\": snippet,\n",
    "                \"relevance_score\": chunk_relevance.output, \n",
    "                \"categories\": [item.strip() for item in chunk_categories.output.split(',')],\n",
    "                \"reasoning_categories\": self.translator(context = extract_reasoning(chunk_categories.rationale)).output,\n",
    "                \"reasoning_relevance\": self.translator(context = extract_reasoning(chunk_relevance.rationale)).output \n",
    "                }\n",
    "            \n",
    "            #Assert categories/classes are correct\n",
    "            #print(chunk_categories.output)\n",
    "            #dspy.Assert(valid_categories([item.strip() for item in chunk_categories.output.split(',')],pred_categories))\n",
    "            \n",
    "            answers.append(entry)\n",
    "        \n",
    "\n",
    "        return dspy.Prediction(context=context, answer=answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = ScanReport()\n",
    "fullana = scan(train_dspy[8].context)\n",
    "#scansnip(context=train_dspy[0]['full_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullana.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_metric(expected: dspy.Example, pred: dspy.Prediction, trace=None) -> int:\n",
    "    \"\"\"Validation metric based on string comparison and regex. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expected : dspy.Example\n",
    "        Expected/example (target) data\n",
    "    pred: dspy.Prediction\n",
    "        Predicted data\n",
    "    trace\n",
    "        If None a score betwen 0 and 1 is returned, else True or False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int/boolean\n",
    "        int: score between 0 and 1 if trace=None\n",
    "        boolean: if trace!=None  \n",
    "    \"\"\"\n",
    "\n",
    "    final_score = 0.0\n",
    "\n",
    "    ## gather quotes and categories\n",
    "    pred_quotes = [text_preprocessing(item['quote']) for item in pred.answer]\n",
    "    pred_relevance = [item['relevance_score'] for item in pred.answer]\n",
    "    pred_categories = [item['categories'] for item in pred.answer]\n",
    "    \n",
    "    expected_quotes = ''.join([text_preprocessing(item['quote']) for item in expected.answer]) # as we already store all quotes in one string\n",
    "    expected_relevance = float([item['relevance_score'] for item in expected.answer][0])\n",
    "    expected_count_quote = float([item['count_quote'] for item in expected.answer][0])\n",
    "    expected_categories = [item['categories'] for item in expected.answer]\n",
    "\n",
    "    quote_match_score = 0.0\n",
    "    categories_match_score = 0.0\n",
    "\n",
    "    # if there are quotes in the target data\n",
    "    if expected_quotes != '':\n",
    "        quote_match_res = quote_match(expected_quotes, expected_count_quote, pred_quotes, pred_relevance)\n",
    "        quote_match_score = quote_match_res[0]\n",
    "        quote_match_indexes = quote_match_res[1]\n",
    "\n",
    "        if len(quote_match_indexes)>0:\n",
    "            categories_match_score, categories_match_size = categories_match(expected_categories,pred_categories, quote_match_indexes) \n",
    "        else: # we assume that expected quotes are exist, but was not matched with the any predicted quotes (llm results)\n",
    "            # print('The quotes match was not found, but the example quote(s) exist(s)')\n",
    "            # look for every predicted category\n",
    "            categories_match_score, categories_match_size = categories_match(expected_categories,pred_categories, range(0,len(pred_categories)))\n",
    "    # else:\n",
    "        # print('The example quote is not found in the target data. No validation for that example.')\n",
    "    \n",
    "    # compute final score \n",
    "    final_score = (quote_match_score+categories_match_score)/2\n",
    "    final_score = min(1.0, final_score)\n",
    "\n",
    "    # add trace for not None (boolean)\n",
    "    if trace != None:\n",
    "        final_score = (quote_match_score > 0.3) & (categories_match_score > 0.3)\n",
    "\n",
    "    return final_score\n",
    "    \n",
    "validation_metric(expected = train_dspy[7] , pred = fullana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=train_dspy[0:13], num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(ScanReport(), metric=validation_metric, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM as Judge Metric \n",
    "\n",
    "# Define metric as softer measure for the reasoning\n",
    "\n",
    "def lm_metric(expected: dspy.Example, pred: dspy.Prediction, trace=None) -> int:\n",
    "    \"\"\"Validation metric based on LLM as a Judge for quotes and \n",
    "    string comparison for categories. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expected : dspy.Example\n",
    "        Expected/example (target) data\n",
    "    pred: dspy.Prediction\n",
    "        Predicted data\n",
    "    trace\n",
    "        If None a score betwen 0 and 1 is returned, else True or False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int/boolean\n",
    "        int: score between 0 and 1 if trace=None\n",
    "        boolean: if trace!=None  \n",
    "    \"\"\"\n",
    "    \n",
    "    ## gather quotes and categories\n",
    "    pred_quotes = [item['quote'] for item in pred.answer]\n",
    "    pred_relevance = [item['relevance_score'] for item in pred.answer]\n",
    "    pred_categories = [item['categories'] for item in pred.answer]\n",
    "    \n",
    "    expected_quotes = ''.join([item['quote'] for item in expected.answer]) # as we already store all quotes in one string\n",
    "    expected_relevance = float([item['relevance_score'] for item in expected.answer][0])\n",
    "    expected_count_quote = float([item['count_quote'] for item in expected.answer][0])\n",
    "    expected_categories = [item['categories'] for item in expected.answer]\n",
    "\n",
    "    # initialize scores\n",
    "    final_score = 0.0\n",
    "    quote_match_score = 0.0\n",
    "    categories_match_score = 0.0\n",
    "\n",
    "    # if there are quotes in the target data\n",
    "    if expected_quotes != '':\n",
    "\n",
    "        # quotes\n",
    "        quote_match_res = quote_match_lm(expected_quotes, expected_count_quote, pred_quotes, pred_relevance)#, lm_gpt)\n",
    "        quote_match_score = quote_match_res[0]\n",
    "        quote_match_indexes = quote_match_res[1]\n",
    "\n",
    "        # categories\n",
    "        if len(quote_match_indexes)>0:\n",
    "            categories_match_score, categories_match_size = categories_match(expected_categories,pred_categories, quote_match_indexes) \n",
    "        else: # we assume that expected quotes are exist, but was not matched with the any predicted quotes (llm results)\n",
    "            # print('The quotes match was not found, but the example quote(s) exist(s)')\n",
    "            # look for every predicted category\n",
    "            categories_match_score, categories_match_size = categories_match(expected_categories,pred_categories, range(0,len(pred_categories)))\n",
    "        # if len(quote_match_indexes)>0:\n",
    "        #     categories_match_score = categories_match_lm(expected_categories, pred_categories, quote_match_indexes) \n",
    "\n",
    "    final_score = (quote_match_score+categories_match_score)/2\n",
    "\n",
    "    # add trace for not None (boolean)\n",
    "    if trace != None:\n",
    "        final_score = (quote_match_score > 0.3) & (categories_match_score > 0.3)\n",
    "\n",
    "    return final_score\n",
    "\n",
    "\n",
    "print(lm_metric(expected = train_dspy[7] , pred = fullana))\n",
    "\n",
    "# def full_lm_metric(expected: dspy.Example, pred: dspy.Prediction, trace=None) -> int:\n",
    "#     \"\"\"Validation metric based purely on LLM as a Judge for \n",
    "#      quotes, categories and reasoning. \n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     expected : dspy.Example\n",
    "#         Expected/example (target) data\n",
    "#     pred : dspy.Prediction\n",
    "#         Predicted data\n",
    "#     trace\n",
    "#         If None a score betwen 0 and 1 is returned, else True or False\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     int/boolean\n",
    "#         int: score between 0 and 1 if trace=None\n",
    "#         boolean: if trace!=None  \n",
    "#     \"\"\"\n",
    "\n",
    "#     ## gather quotes and categories\n",
    "#     pred_quotes = [item['quote'] for item in pred.answer]\n",
    "#     pred_relevance = [item['relevance_score'] for item in pred.answer]\n",
    "#     pred_categories = [item['categories'] for item in pred.answer]\n",
    "#     pred_reasoning_relevance = [item['reasoning_relevance'] for item in pred.answer]\n",
    "\n",
    "#     expected_quotes = ''.join([item['quote'] for item in expected.answer]) # as we already store all quotes in one string\n",
    "#     expected_relevance = float([item['relevance_score'] for item in expected.answer][0])\n",
    "#     expected_count_quote = float([item['count_quote'] for item in expected.answer][0])\n",
    "#     expected_categories = [item['categories'] for item in expected.answer]\n",
    "\n",
    "#     # initialize scores\n",
    "#     final_score = 0.0\n",
    "#     quote_match_score = 0.0\n",
    "#     categories_match_score = 0.0\n",
    "\n",
    "#     reasoning_relevance_score = relevance_reasoning_lm(pred_quotes, pred_reasoning_relevance, pred_relevance, lm_gpt)\n",
    "\n",
    "#     # if there are quotes in the target data\n",
    "#     if expected_quotes != '':\n",
    "\n",
    "#         quote_match_res = quote_match_lm(expected_quotes, expected_count_quote, pred_quotes, pred_relevance, lm_gpt)\n",
    "#         quote_match_score = quote_match_res[0]\n",
    "#         quote_match_indexes = quote_match_res[1]\n",
    "\n",
    "#         if len(quote_match_indexes)>0:\n",
    "#             categories_match_score = categories_match_lm(expected_categories, pred_categories, quote_match_indexes, lm_gpt) \n",
    "\n",
    "#     else:\n",
    "#         # print('The example quote is not found in the target data. No validation for that example.')\n",
    "#         quote_match_score = 0.0\n",
    "\n",
    "#         for pred_item in pred_relevance:\n",
    "#             # print(pred_item)\n",
    "#             try:\n",
    "#                 pred_item = (100 - float(pred_item))/100\n",
    "#             except:\n",
    "#                 pred_item = 0.5\n",
    "#             quote_match_score = quote_match_score + pred_item   \n",
    "        \n",
    "#         quote_match_score = quote_match_score/len(pred_relevance)\n",
    "    \n",
    "#     final_score = ((2*quote_match_score) + (2*categories_match_score) + reasoning_relevance_score) / 5\n",
    "\n",
    "#     # add trace for not None (boolean)\n",
    "#     if trace != None:\n",
    "#         final_score = (quote_match_score > 0.3) & (categories_match_score > 0.3) & (reasoning_relevance_score > 0.3)\n",
    "\n",
    "#     return final_score\n",
    "\n",
    "\n",
    "# print(full_lm_metric(expected = train_dspy[1] , pred = fullana))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=train_dspy[0:13], num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(ScanReport(), metric=lm_metric, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## consider compiling with BootstrapFewShot with reasoning and relevance score \n",
    "from teleprompt.bootstrap import BootstrapFewShot\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our program.\n",
    "teleprompter = BootstrapFewShot(metric=validation_metric, max_rounds=4,max_bootstrapped_demos=6, max_errors=1)\n",
    "\n",
    "# Compile BootstrapFewShot\n",
    "compiled_ScanReport = teleprompter.compile(ScanReport(), trainset=train_dspy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## consider compiling with COPRO \n",
    "from teleprompt.copro_optimizer import COPRO\n",
    "\n",
    "# Set COPRO\n",
    "teleprompter = COPRO(metric=validation_metric, breadth=2, depth=2, init_temperature=0.9)\n",
    "\n",
    "eval_dict = {\n",
    "    \"display_progress\": True,\n",
    "    \"display_table\": 0\n",
    "    }\n",
    "\n",
    "# Compile COPRO\n",
    "compiled_ScanReport = teleprompter.compile(student = ScanReport(), trainset=train_dspy[0:5], eval_kwargs = eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPRO and BootstrapFewShot combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## consider compiling with BootstrapFewShot with reasoning and relevance score \n",
    "from teleprompt.bootstrap import BootstrapFewShot\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our program.\n",
    "teleprompter = BootstrapFewShot(metric=validation_metric, max_rounds=4,max_bootstrapped_demos=5, max_errors=1)\n",
    "\n",
    "# Compile BootstrapFewShot\n",
    "compiled_ScanReport = teleprompter.compile(compiled_ScanReport, trainset=train_dspy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## consider compiling with two optimizers\n",
    "from teleprompt.copro_optimizer import COPRO\n",
    "from teleprompt.bootstrap import BootstrapFewShot\n",
    "\n",
    "opt_metric = validation_metric\n",
    "\n",
    "# Set up COPRO\n",
    "teleprompter = COPRO(metric=opt_metric, breadth=2, depth=2, init_temperature=0.9)\n",
    "eval_dict = {\n",
    "    \"display_progress\": False,\n",
    "    \"display_table\": 0\n",
    "    }\n",
    "# Compile COPRO\n",
    "compiled_ScanReport1 = teleprompter.compile(student = ScanReport(), trainset=train_dspy[0:6], eval_kwargs = eval_dict)\n",
    "\n",
    "print(\"COPRO round 1 done, optimize with BootstrapFewShot\")\n",
    "\n",
    "# Set up BootstrapFewShot\n",
    "teleprompter = BootstrapFewShot(metric=opt_metric, max_rounds=4,max_bootstrapped_demos=6, max_errors=1)\n",
    "# Compile BootstrapFewShot\n",
    "compiled_ScanReport2 = teleprompter.compile(compiled_ScanReport1, trainset=train_dspy)\n",
    "\n",
    "print(\"BootstrapFewShot done, optimize with COPRO round 2\")\n",
    "\n",
    "# again COPRO\n",
    "teleprompter = COPRO(metric=opt_metric, breadth=2, depth=2, init_temperature=0.9)\n",
    "compiled_ScanReport = teleprompter.compile(student = compiled_ScanReport2, trainset=train_dspy[7:13], eval_kwargs = eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=train_dspy[0:13], num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(compiled_ScanReport, metric=validation_metric, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=train_dspy[0:13], num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(compiled_ScanReport, metric=lm_metric, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=val_dspy[0:5], num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(ScanReport(), metric=validation_metric, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=val_dspy[0:5], num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(ScanReport(), metric=lm_metric, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=val_dspy[0:5], num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(compiled_ScanReport, metric=validation_metric, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=val_dspy[0:5], num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(compiled_ScanReport, metric=lm_metric, return_all_scores=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
